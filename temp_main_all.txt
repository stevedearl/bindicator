from fastapi import FastAPI, Query, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from datetime import date, timedelta, datetime, timezone
from pydantic import BaseModel, Field, ConfigDict
from enum import Enum
from typing import List, Dict
import os
import threading
import asyncio
import logging
from fastapi.responses import JSONResponse
import time
import random
# Import cache module in a way that works both when running as a script
# (python backend/main.py) and as a package (uvicorn backend.main:app)
try:
    from . import cache as disk_cache  # type: ignore
except Exception:
    try:
        from backend import cache as disk_cache  # type: ignore
    except Exception:
        import cache as disk_cache  # type: ignore


# Logging setup with timestamps
_LOG_LEVEL = getattr(logging, os.getenv("BINDICATOR_LOG_LEVEL", "INFO").upper(), logging.INFO)
logging.basicConfig(
    level=_LOG_LEVEL,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
log = logging.getLogger("bindicator")

app = FastAPI(title="Bindicator API", version="0.1.0")

# Prefetch telemetry
LAST_PREFETCH_AT: datetime | None = None
PREFETCH_STATS: Dict[str, int] = {"attempted": 0, "refreshed": 0, "failed": 0}

# CORS for local dev (frontend on Vite dev server)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/api/health")
def health():
    # Summarize disk cache state
    try:
        cached = disk_cache.iter_cached_postcodes()
    except Exception:
        cached = {}
    mixed = []
    for k, v in cached.items():
        if v.get("mixed_routes") is True:
            mixed.append({
                "postcode": k,
                "checked_at": v.get("mixed_routes_checked_at"),
                "addresses": list((v.get("mixed_routes_details") or {}).keys()),
            })
    postcodes = list(cached.keys())[:10]
    return {
        "status": "ok",
        "datasource": os.getenv("BINDICATOR_DATASOURCE", "mock").lower(),
        "cache": {
            "entries": len(cached),
            "postcodes": postcodes,
            "mixed_routes": mixed,
            "lastPrefetchAt": LAST_PREFETCH_AT.isoformat() if LAST_PREFETCH_AT else None,
            "prefetchStats": PREFETCH_STATS,
        },
    }


# Friendly error responses
@app.exception_handler(HTTPException)
def http_exception_handler(_, exc: HTTPException):
    datasource = os.getenv("BINDICATOR_DATASOURCE", "mock").lower()
    hint = None
    if exc.status_code == 502:
        hint = (
            "RBWM may be busy or unavailable. Please try again in a minute, "
            "or switch datasource to 'mock' for testing."
        )
    payload = {
        "error": exc.detail or "HTTP error",
        "code": exc.status_code,
        "datasource": datasource,
        "hint": hint,
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }
    return JSONResponse(status_code=exc.status_code, content=payload)


@app.exception_handler(Exception)
def unhandled_exception_handler(_, exc: Exception):
    log.exception("Unhandled server error")
    payload = {
        "error": "Internal server error",
        "code": 500,
        "datasource": os.getenv("BINDICATOR_DATASOURCE", "mock").lower(),
        "hint": "Please retry shortly. If this persists, check server logs.",
        "timestamp": datetime.now(timezone.utc).isoformat(),
    }
    return JSONResponse(status_code=500, content=payload)


class BinType(str, Enum):
    blue = "blue"      # Recycling
    green = "green"    # Garden
    black = "black"    # Rubbish


class ScraperResult(BaseModel):
    """Contract for the scraper output before transformation to API shape."""
    postcode: str
    next_collection_date: date | None
    bins: List[BinType]
    # Optional raw fields to aid debugging can be added later (e.g., raw_html)


class BinResponse(BaseModel):
    """Public API response model (camelCase via aliases)."""
    model_config = ConfigDict(populate_by_name=True)

    postcode: str
    next_collection_date: date | None = Field(alias="nextCollectionDate")
    next_collection_day: str | None = Field(alias="nextCollectionDay")
    bins: List[BinType]
    source: str
    cached: bool = False
    fetched_at: datetime = Field(alias="fetchedAt")
    mixed_routes: bool | None = Field(default=None, alias="mixed_routes")
    addresses: List[str] | None = None
    no_collections: bool = Field(default=False, alias="noCollections")


def _normalize_postcode(pc: str) -> str:
    return pc.strip().replace(" ", "").upper()


def _weekday_name(d: date) -> str:
    return d.strftime("%A")


def scrape_rbwm_schedule(postcode: str) -> ScraperResult:
    """RBWM provider wrapper: uses Playwright when datasource is set to 'rbwm'.
    Falls back to mock if not configured or on error.
    """
    datasource = os.getenv("BINDICATOR_DATASOURCE", "mock").lower()
    key = _normalize_postcode(postcode)
    if datasource == "rbwm":
        try:
            # Import in a way that works both as package and as script
            try:
                from backend.scraper.rbwm import fetch_rbwm_schedule_autoselect as _async_fetch  # type: ignore
            except Exception:
                import sys as _sys, os as _os
                # Ensure backend/ is on sys.path to import sibling package 'scraper'
                backend_dir = _os.path.dirname(__file__)
                if backend_dir not in _sys.path:
                    _sys.path.insert(0, backend_dir)
                from scraper.rbwm import fetch_rbwm_schedule_autoselect as _async_fetch  # type: ignore

            # Run async Playwright scraper in this sync context
            return asyncio.run(_async_fetch(key))
        except Exception:
            # In RBWM mode, do not fall back to mock â€” surface an upstream failure
            log.exception("RBWM postcode scrape failed")
            raise

    # Mock fallback
    # If key is empty or doesn't end with a digit, choose a deterministic default
    if key and key[-1].isdigit():
        even = int(key[-1]) % 2 == 0
    else:
        # Use current day ordinal parity for deterministic variation
        even = (date.today().toordinal() % 2 == 0)
    today = date.today()
    next_collection = today + timedelta(days=(2 if even else 3))
    # Mock follows RBWM rule: always blue + (black or green)
    bins = [BinType.blue, (BinType.black if even else BinType.green)]
    return ScraperResult(postcode=key, next_collection_date=next_collection, bins=bins)


def build_response_from_scrape(scrape: ScraperResult, *, source: str, cached: bool) -> BinResponse:
    return BinResponse(
        postcode=scrape.postcode,
        next_collection_date=scrape.next_collection_date,
        next_collection_day=_weekday_name(scrape.next_collection_date) if scrape.next_collection_date else None,
        bins=scrape.bins,
        source=source,
        cached=cached,
        fetched_at=datetime.now(timezone.utc),
        no_collections=(scrape.next_collection_date is None or len(scrape.bins) == 0),
    )


@app.get("/api/bins", response_model=BinResponse, response_model_by_alias=True)
def get_bins(
    postcode: str | None = Query(None, min_length=5, max_length=10),
    uprn: str | None = Query(None, description="RBWM Unique Property Reference Number"),
    refresh: bool = Query(False, description="Force refresh ignoring cache"),
):
    """
    Returns next collection info.
    - If `uprn` is provided and datasource=rbwm, fetch by UPRN (preferred).
    - Else if `postcode` is provided, use rbwm/mock postcode flow.
    Uses in-memory cache; set `refresh=true` to bypass.
    """
    if uprn:
        cache_key = f"uprn:{uprn}"
    elif postcode:
        cache_key = f"pc:{_normalize_postcode(postcode)}"
    else:
        raise HTTPException(status_code=400, detail="Provide either 'uprn' or 'postcode'")

    # No separate in-memory caching; prefer disk cache for postcode lookups

    datasource = os.getenv("BINDICATOR_DATASOURCE", "mock").lower()
    if uprn and datasource == "rbwm":
        # Disk cache for UPRN (same-day)
        if not refresh:
            try:
                key = f"uprn:{uprn}"
                if disk_cache.is_same_day_cached_key(key):
                    item = disk_cache.get_cached_key(key)
                    if item and isinstance(item.get("data"), dict):
                        data = dict(item["data"])  # shallow copy
                        data["cached"] = True
                        log.info("[cache] Hit for %s (same-day data).", key)
                        return data
            except Exception:
                log.exception("Disk UPRN cache read failed")
        try:
            # Try fast HTTP path first
            from backend.scraper.rbwm import fetch_rbwm_schedule_by_uprn_http as _fetch_http
            scrape = _fetch_http(uprn)
            source = "rbwm"
        except Exception:
            log.exception("RBWM UPRN HTTP fetch failed; trying Playwright")
            try:
                from backend.scraper.rbwm import fetch_rbwm_schedule_by_uprn as _fetch_by_uprn
                scrape = asyncio.run(_fetch_by_uprn(uprn))
                source = "rbwm"
            except Exception:
                log.exception("RBWM UPRN Playwright failed; returning error (no mock fallback in rbwm mode)")
                raise HTTPException(status_code=502, detail="RBWM upstream fetch failed for UPRN")
    else:
        pc_norm = _normalize_postcode(postcode or "")
        # Persistent on-disk cache only applies to postcode lookups
        # Check disk cache (same-day validation) unless refresh=true
        if postcode and not refresh:
            try:
                if disk_cache.is_same_day_cached(postcode):
                    item = disk_cache.get_cached(postcode)
                    if item and isinstance(item.get("data"), dict):
                        data = dict(item["data"])  # shallow copy
                        data["cached"] = True
                        # propagate verification flags
                        if item.get("mixed_routes") is True:
                            data["mixed_routes"] = True
                            details = item.get("mixed_routes_details") or {}
                            data["addresses"] = list(details.keys()) if isinstance(details, dict) else None
                        log.info("[cache] Hit for %s (same-day data).", item.get("key", postcode))
                        return data
            except Exception:
                log.exception("Disk cache read failed")

        if datasource == "rbwm":
            # Smart-hybrid postcode flow:
            # 1) Try pure-HTTP path: first address -> schedule (with one polite retry)
            # 2) Fallback to Playwright auto-select
            # If both fail, surface 502
            try:
                log.info("[cache] Refreshing %s (new day or refresh=true).", postcode)
                try:
                    from backend.scraper.rbwm import (
                        fetch_rbwm_addresses_http as _addr_http,
                        fetch_rbwm_schedule_by_uprn_http as _sched_http,
                    )
                except Exception:
                    # Allow running as a script: import sibling 'scraper' by path
                    import sys as _sys, os as _os
                    backend_dir = _os.path.dirname(__file__)
                    if backend_dir not in _sys.path:
                        _sys.path.insert(0, backend_dir)
                    from scraper.rbwm import (  # type: ignore
                        fetch_rbwm_addresses_http as _addr_http,
                        fetch_rbwm_schedule_by_uprn_http as _sched_http,
                    )

                addrs = _addr_http(postcode)
                if not addrs:
                    time.sleep(random.uniform(0.9, 1.8))
                    addrs = _addr_http(postcode)
                if not addrs:
                    raise RuntimeError("no addresses from HTTP")
                first = addrs[0]
                log.info("[scraper] HTTP first address for %s: %s (%s)", postcode, first.address, first.uprn)
                scrape = _sched_http(first.uprn)
                source = "rbwm"
            except Exception:
                log.exception("RBWM HTTP postcode path failed; trying Playwright autoselect")
                try:
                    scrape = scrape_rbwm_schedule(pc_norm)
                    source = "rbwm"
                except Exception:
                    log.exception("RBWM postcode fetch failed; returning error (no mock fallback in rbwm mode)")
                    raise HTTPException(status_code=502, detail="RBWM upstream fetch failed for postcode")
        else:
            log.info("[cache] Refreshing %s (mock mode).", postcode)
            scrape = scrape_rbwm_schedule(pc_norm)
            source = "mock"

    resp = build_response_from_scrape(scrape, source=source, cached=False)
    # Persist to disk cache
    try:
        if postcode:
            # store response as plain dict with alias keys
            disk_cache.update_cache(postcode, {
                "postcode": resp.postcode,
                "nextCollectionDate": resp.next_collection_date.isoformat() if resp.next_collection_date else None,
                "nextCollectionDay": resp.next_collection_day,
                "bins": [b.value for b in resp.bins],
                "source": resp.source,
                "cached": False,
                "fetchedAt": resp.fetched_at.isoformat(),
                "noCollections": resp.no_collections,
            }, mixed_routes=None, mixed_routes_checked=False)
        elif uprn:
            disk_cache.update_cache_key(f"uprn:{uprn}", {
                "postcode": resp.postcode,
                "nextCollectionDate": resp.next_collection_date.isoformat() if resp.next_collection_date else None,
                "nextCollectionDay": resp.next_collection_day,
                "bins": [b.value for b in resp.bins],
                "source": resp.source,
                "cached": False,
                "fetchedAt": resp.fetched_at.isoformat(),
                "noCollections": resp.no_collections,
            })
    except Exception:
        log.exception("Disk cache write failed")

    return resp


class AddressItem(BaseModel):
    uprn: str
    address: str


@app.get("/api/addresses", response_model=List[AddressItem])
def get_addresses(postcode: str = Query(..., min_length=5, max_length=10)):
    """RBWM address lookup: returns a list of UPRNs for a postcode.
    Requires BINDICATOR_DATASOURCE=rbwm.
    """
    datasource = os.getenv("BINDICATOR_DATASOURCE", "mock").lower()
    if datasource != "rbwm":
        return []

    try:
        # Try fast HTTP path first
        try:
            from backend.scraper.rbwm import fetch_rbwm_addresses_http as _fetch_http
        except Exception:
            # Allow running as a script: import sibling 'scraper' by path
            import sys as _sys, os as _os
            backend_dir = _os.path.dirname(__file__)
            if backend_dir not in _sys.path:
                _sys.path.insert(0, backend_dir)
            from scraper.rbwm import fetch_rbwm_addresses_http as _fetch_http
        results = _fetch_http(postcode)
        if not results:
            raise RuntimeError("No addresses found via HTTP")
        addrs = [AddressItem(uprn=r.uprn, address=r.address) for r in results]
        log.info("RBWM HTTP addresses: %s candidates for %s", len(addrs), postcode)
        return addrs
    except Exception:
        log.exception("RBWM address HTTP lookup failed; trying Playwright")
        try:
            try:
                from backend.scraper.rbwm import fetch_rbwm_addresses as _fetch_addrs
            except Exception:
                import sys as _sys, os as _os
                backend_dir = _os.path.dirname(__file__)
                if backend_dir not in _sys.path:
                    _sys.path.insert(0, backend_dir)
                from scraper.rbwm import fetch_rbwm_addresses as _fetch_addrs
            results = asyncio.run(_fetch_addrs(postcode))
            addrs = [AddressItem(uprn=r.uprn, address=r.address) for r in results]
            log.info("RBWM Playwright addresses: %s candidates for %s", len(addrs), postcode)
            return addrs
        except Exception:
            log.exception("RBWM address lookup failed")
            return []


# --- Cache admin endpoints (dev convenience) ---
class CacheStatus(BaseModel):
    entries: int
    keys: List[str]
    now: datetime


@app.get("/api/cache/status", response_model=CacheStatus)
def cache_status(limit: int = Query(10, ge=0, le=100)):
    now = datetime.now(timezone.utc)
    try:
        entries = disk_cache.iter_cached_postcodes()
        keys = list(entries.keys())[:limit]
        return CacheStatus(entries=len(entries), keys=keys, now=now)
    except Exception:
        log.exception("Cache status failed")
        return CacheStatus(entries=0, keys=[], now=now)


@app.post("/api/cache/clear")
def cache_clear(scope: str | None = Query(None, description="all|uprn|pc"), key: str | None = Query(None)):
    if key:
        ok = disk_cache.delete_key(key)
        return {"removed": 1 if ok else 0}
    if scope in {"uprn", "pc"}:
        removed = disk_cache.delete_scope(scope + ":")
        return {"removed": removed}
    # clear all
    entries = disk_cache.iter_cached_postcodes()
    removed = 0
    for k in list(entries.keys()):
        if disk_cache.delete_key(k):
            removed += 1
    return {"removed": removed}


class ResolvedAddress(BaseModel):
    uprn: str
    address: str
    exact: bool = False
    score: int = 0


def _score_address_match(address: str, house: str) -> tuple[int, bool]:
    """Return a score and exact flag for how well the address matches the house input.
    Higher score is better. Exact means starts with the same house token (e.g., '22' or '22A').
    """
    a = address.strip().lower()
    h = house.strip().lower()
    if not a or not h:
        return (0, False)
    # Exact if the first token starts with the house string (to allow 22A)
    first_seg = a.split(',')[0].strip()
    first_parts = first_seg.split()
    first = first_parts[0] if first_parts else ""
    exact = first.startswith(h)
    score = 0
    if exact:
        score += 100
        if first == h:
            score += 20
    # Bonus if the house number appears at the start of line
    if a.startswith(h + ' '):
        score += 10
    # Minor bonus if contains elsewhere
    if (' ' + h + ' ') in a:
        score += 2
    return (score, exact)


@app.get("/api/resolve", response_model=List[ResolvedAddress])
def resolve_address(
    postcode: str = Query(..., min_length=5, max_length=10),
    house: str | None = Query(None, description="House number/name to match"),
):
    """Resolve a postcode and house query to one or more RBWM UPRNs.
    Returns sorted candidates with a score and exact flag. Requires datasource=rbwm.
    """
    datasource = os.getenv("BINDICATOR_DATASOURCE", "mock").lower()
    if datasource != "rbwm":
        return []

    # Use HTTP path for speed; fall back to Playwright if needed
    def _fetch_all() -> List[AddressItem]:
        from backend.scraper.rbwm import fetch_rbwm_addresses_http as _fetch_http
        try:
            res = _fetch_http(postcode)
            if res:
                return [AddressItem(uprn=r.uprn, address=r.address) for r in res]
            raise RuntimeError("no addresses via http")
        except Exception:
            from backend.scraper.rbwm import fetch_rbwm_addresses as _fetch_pw
            try:
                res2 = asyncio.run(_fetch_pw(postcode))
                return [AddressItem(uprn=r.uprn, address=r.address) for r in res2]
            except Exception:
                log.exception("Resolve: address lookup failed")
                return []


@app.get("/api/debug/lazy-verify")
def lazy_verify(postcode: str = Query(..., min_length=5, max_length=10)):
    if os.getenv("BINDICATOR_DEBUG", "false").lower() not in {"1", "true", "yes", "on"}:
        raise HTTPException(status_code=404, detail="Not found")
    # throttle
    if disk_cache.should_throttle_verify(postcode, hours=24):
        entry = disk_cache.get_entry(postcode) or {}
        return {
            "postcode": postcode.upper(),
            "mixed_routes": entry.get("mixed_routes"),
            "checked_at": entry.get("mixed_routes_checked_at"),
            "throttled": True,
        }

    # run verification (RBWM only)
    datasource = os.getenv("BINDICATOR_DATASOURCE", "mock").lower()
    if datasource != "rbwm":
        raise HTTPException(status_code=400, detail="Lazy verify available only in rbwm mode")

    try:
        from backend.scraper.rbwm import verify_postcode_consistency as _verify
        result = asyncio.run(_verify(postcode))
        mixed = not bool(result.get("consistent"))
        details = result.get("differences") if mixed else {}
        disk_cache.update_verification(postcode, mixed_routes=mixed, details=details)
        log.info("[verify] Lazy verification complete for %s (mixed_routes=%s)", postcode.upper(), mixed)
        entry = disk_cache.get_entry(postcode) or {}
        return {
            "postcode": postcode.upper(),
            "mixed_routes": entry.get("mixed_routes"),
            "checked_at": entry.get("mixed_routes_checked_at"),
            "addresses": list((entry.get("mixed_routes_details") or {}).keys()),
            "throttled": False,
        }
    except Exception:
        log.exception("Lazy verification failed for %s", postcode)
        raise HTTPException(status_code=502, detail="Verification failed")

    # Always ensure we work with a list
    items = _fetch_all() or []
    # If no house provided, return a simple shortlist (no scoring)
    if not (house or "").strip():
        return [ResolvedAddress(uprn=it.uprn, address=it.address, exact=False, score=0) for it in items[:10]]
    scored: List[ResolvedAddress] = []
    for it in items:
        s, exact = _score_address_match(it.address, house)
        if s > 0:
            scored.append(ResolvedAddress(uprn=it.uprn, address=it.address, exact=exact, score=s))

    # If nothing matched by score, return the raw list (limited) to allow manual choice
    if not scored:
        return [ResolvedAddress(uprn=it.uprn, address=it.address, exact=False, score=0) for it in items[:10]]

    # Fallback: ensure a list is always returned
    if scored is None:
        return []

    # Sort by exact desc, score desc, then address asc
    scored.sort(key=lambda x: (x.exact, x.score, x.address.lower()), reverse=True)
    # Return top 10
    return scored[:10]


if __name__ == "__main__":
    import uvicorn
    host = os.getenv("HOST", "127.0.0.1")
    port = int(os.getenv("PORT", "8000"))
    # Load cache and prefetch stale entries in background
    try:
        disk_cache.load_cache()
    except Exception:
        log.exception("Failed to load disk cache on startup")

    def _prefetch():
        entries = disk_cache.iter_cached_postcodes()
        today = datetime.now(timezone.utc).date()
        PREFETCH_STATS.update({"attempted": 0, "refreshed": 0, "failed": 0})
        for key, item in entries.items():
            try:
                ts = item.get("fetched_at")
                d = datetime.fromisoformat(str(ts).replace("Z", "+00:00")).date() if ts else None
                if d != today:
                    # Refresh
                    log.info("[cache] Prefetch refreshing %s (stale)", key)
                    PREFETCH_STATS["attempted"] += 1
                    pc_norm = _normalize_postcode(key)
                    datasource = os.getenv("BINDICATOR_DATASOURCE", "mock").lower()
                    if datasource == "rbwm":
                        # Prefer HTTP path during prefetch (lighter, no headless)
                        try:
                            try:
                                from backend.scraper.rbwm import (
                                    fetch_rbwm_addresses_http as _addr_http,
                                    fetch_rbwm_schedule_by_uprn_http as _sched_http,
                                )
                            except Exception:
                                # Ensure backend dir on path to import sibling package 'scraper'
                                import sys as _sys, os as _os
                                backend_dir = _os.path.dirname(__file__)
                                if backend_dir not in _sys.path:
                                    _sys.path.insert(0, backend_dir)
                                from scraper.rbwm import (
                                    fetch_rbwm_addresses_http as _addr_http,
                                    fetch_rbwm_schedule_by_uprn_http as _sched_http,
                                )

                            addrs = _addr_http(key)
                            if not addrs:
                                # Polite small wait and retry once
                                time.sleep(random.uniform(1.0, 2.0))
                                addrs = _addr_http(key)
                            if not addrs:
                                raise RuntimeError("no addresses via http")
                            first = addrs[0]
                            sc = _sched_http(first.uprn)
                            res = build_response_from_scrape(sc, source="rbwm", cached=False)
                            PREFETCH_STATS["refreshed"] += 1
                        except Exception:
                            # Fall back to Playwright autoselect once; if it fails, log and skip
                            try:
                                time.sleep(random.uniform(0.8, 1.6))
                                sc = scrape_rbwm_schedule(key)
                                res = build_response_from_scrape(sc, source="rbwm", cached=False)
                                PREFETCH_STATS["refreshed"] += 1
                            except Exception:
                                log.warning("Prefetch RBWM failed for %s (skipping)", key)
                                PREFETCH_STATS["failed"] += 1
                                continue
                    else:
                        sc = scrape_rbwm_schedule(key)
                        res = build_response_from_scrape(sc, source="mock", cached=False)
                        PREFETCH_STATS["refreshed"] += 1
                    disk_cache.update_cache(key, {
                        "postcode": res.postcode,
                        "nextCollectionDate": res.next_collection_date.isoformat() if res.next_collection_date else None,
                        "nextCollectionDay": res.next_collection_day,
                        "bins": [b.value for b in res.bins],
                        "source": res.source,
                        "cached": False,
                        "fetchedAt": res.fetched_at.isoformat(),
                        "noCollections": res.no_collections,
                    })
            except Exception:
                log.exception("Prefetch processing failed for %s", key)
                PREFETCH_STATS["failed"] += 1
        global LAST_PREFETCH_AT
        LAST_PREFETCH_AT = datetime.now(timezone.utc)

    threading.Thread(target=_prefetch, daemon=True).start()

    uvicorn.run(app, host=host, port=port)
